import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import re
import streamlit as st
from nltk.tokenize import RegexpTokenizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from PIL import Image

# í•œê¸€ í°íŠ¸ ì„¤ì • (Streamlitì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ë¶ˆìš©ì–´ íŒŒì¼ ê²½ë¡œ (ì§ì ‘ ê²½ë¡œ ì„¤ì •)
stopwords_path = "C:/Users/host/Desktop/PROJECT_SeSAC_02/datas/stopwords.txt"  # ê²½ë¡œë¥¼ ì—¬ê¸°ì„œ ì„¤ì •

# ì¹´í…Œê³ ë¦¬ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” sid ê°’ ì„¤ì •
category_sid_map = {
    "ì •ì¹˜": "100",
    "ê²½ì œ": "101",
    "ì‚¬íšŒ": "102",
    "ìƒí™œ/ë¬¸í™”": "103",
    "IT/ê³¼í•™": "105",
    "ì„¸ê³„": "104"
}

# ë¶ˆìš©ì–´ íŒŒì¼ì„ ì½ì–´ë“¤ì´ëŠ” í•¨ìˆ˜ (ê²½ë¡œë¥¼ ì§ì ‘ ì‚¬ìš©)
def load_stopwords(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            stop_words = set(line.strip() for line in f)
        return stop_words
    except FileNotFoundError:
        st.warning(f"ë¶ˆìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return set()

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text, stopwords):
    tokenizer = RegexpTokenizer(r'\w+')
    text = re.sub(r'\[.*?\]', '', text)  # ëŒ€ê´„í˜¸ ë‚´ìš© ì œê±°
    text = re.sub(r'\d+', '', text)  # ìˆ«ì ì œê±°
    tokens = tokenizer.tokenize(text)  # í† í°í™”
    tokens = [word for word in tokens if word not in stopwords]  # ë¶ˆìš©ì–´ ì œê±°
    return tokens

# ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def fetch_page(url, headers, params):
    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()
    return response.text

def parse_html(html):
    soup = BeautifulSoup(html, "html.parser")
    articles = soup.find_all("a", attrs={"data-clk": True})
    article_urls = [article.get("href") for article in articles if article.get("href")]
    div_tag = soup.find("div", attrs={"data-cursor": True})
    next_cursor = div_tag.get("data-cursor", "").strip('"') if div_tag else None
    article_urls = list(set(article_urls))
    return article_urls, next_cursor

def fetch_article_details(article_url):
    article_url = re.sub(r'[\\"]', '', article_url)
    if article_url.startswith("/"):
        article_url = "https://n.news.naver.com" + article_url
    response = requests.get(article_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_title = soup.select_one('h2#title_area').text.strip() if soup.select_one('h2#title_area') else 'ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
    news_article = soup.select_one('div#newsct_article').text.strip() if soup.select_one('div#newsct_article') else 'ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
    return news_title, news_article

# í¬ë¡¤ë§ì„ í†µí•´ ìµœëŒ€ 200ê°œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
def crawl_news(max_articles=200):
    articles = []
    page_no = 1
    
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸°
    category = st.session_state.get("category", "ì •ì¹˜")  # ê¸°ë³¸ê°’ì€ ì •ì¹˜
    sid = category_sid_map.get(category, "100")  # ê¸°ë³¸ê°’ì€ ì •ì¹˜(100)ìœ¼ë¡œ ì„¤ì •

    url = "https://news.naver.com/section/template/SECTION_ARTICLE_LIST"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36",
    }

    current_time = datetime.now().strftime("%Y%m%d%H%M%S")
    params = {
        "sid": sid,
        "pageNo": "1",
        "next": current_time,
        "_": str(int(time.time())),
    }

    while len(articles) < max_articles:
        params["pageNo"] = str(page_no)
        html = fetch_page(url, headers, params)
        article_urls, next_cursor = parse_html(html)

        for article_url in article_urls:
            title, content = fetch_article_details(article_url)
            preprocessed_content = preprocess_text(content, stopwords)
            articles.append(" ".join(preprocessed_content))
            if len(articles) >= max_articles:
                break

        page_no += 1
        
        if not article_urls:
            break

    return articles[:max_articles]  # ìµœëŒ€ max_articles ê°œìˆ˜ë§Œ ë°˜í™˜

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜ (ë°°ê²½ ê²€ì •ìƒ‰, JPGë¡œ ì €ì¥)
def generate_wordcloud(text_data):
    text = " ".join(text_data)
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼ ì„¤ì •
    wordcloud = WordCloud(
        font_path="C:/Windows/Fonts/malgun.ttf",  # í•œê¸€ í°íŠ¸ ê²½ë¡œ
        width=1200,  # í¬ê¸° í‚¤ì›€
        height=600,  # í¬ê¸° í‚¤ì›€
        background_color="black",  # ë°°ê²½ì„ ê²€ì •ìƒ‰ìœ¼ë¡œ ì„¤ì •
        mode="RGB",  # RGB ëª¨ë“œ ì„¤ì •
        colormap="Spectral",  # ìƒ‰ìƒ ì„¤ì •
        contour_color=None,  # í…Œë‘ë¦¬ ìƒ‰ìƒ ì œê±°
        max_words=70,  # ìµœëŒ€ ë‹¨ì–´ ìˆ˜
    ).generate(text)
    
    # ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (RGB ëª¨ë“œ)
    img = wordcloud.to_image()

    # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥ (JPG í˜•ì‹)
    img_path = "C:/Users/host/Desktop/PROJECT_SeSAC_02/datas/wordcloud_output.jpg"
    img.save(img_path, 'JPEG')

    # ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì½ì–´ Streamlitì— í‘œì‹œ
    st.image(img_path, use_column_width=True)

# í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜
def crawling_function_sub2():
    global stopwords
    stopwords = load_stopwords(stopwords_path)  # ë¶ˆìš©ì–´ ë¡œë“œ

    if stopwords:
        
        # ì„¸ì…˜ ìƒíƒœì—ì„œ ì…ë ¥ê°’ ê°€ì ¸ì˜¤ê¸°
        category = st.session_state.get("category", "ì •ì¹˜")  # ê¸°ë³¸ê°’ì€ ì •ì¹˜
        max_articles = st.session_state.get("max_articles", 50)  # ê¸°ë³¸ê°’ì€ 50

        st.write(f"ğŸ‘‰ì¹´í…Œê³ ë¦¬ : {category}")
        st.write(f"ğŸ‘‰í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜ : {max_articles}")

        articles = crawl_news(max_articles)  # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        generate_wordcloud(articles)

    else:
        st.warning("ë¶ˆìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
